{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Op9B9Rs0b_TP"
   },
   "source": [
    "# 03 Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect to GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 552,
     "status": "ok",
     "timestamp": 1615989340816,
     "user": {
      "displayName": "Manuela Paganini",
      "photoUrl": "",
      "userId": "16245217794239939786"
     },
     "user_tz": -60
    },
    "id": "DEW4yvH5b_Fo",
    "outputId": "8c27257b-fd4f-4c47-95d1-26f252fed80c"
   },
   "outputs": [],
   "source": [
    "# set up connection to your google drive\n",
    "# please click on the link generated and enter the authorisation code\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 487,
     "status": "ok",
     "timestamp": 1615989336681,
     "user": {
      "displayName": "Manuela Paganini",
      "photoUrl": "",
      "userId": "16245217794239939786"
     },
     "user_tz": -60
    },
    "id": "gRFDCydJb-61",
    "outputId": "236de9bc-68d8-41c9-bf7e-9a0d3d62c3be"
   },
   "outputs": [],
   "source": [
    "cd /content/drive/MyDrive/Colab/statbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26952,
     "status": "ok",
     "timestamp": 1615988958604,
     "user": {
      "displayName": "Manuela Paganini",
      "photoUrl": "",
      "userId": "16245217794239939786"
     },
     "user_tz": -60
    },
    "id": "V7DxBQscb-sn",
    "outputId": "bdc3858b-5a7c-46f9-cf2d-4e1f5b90c07f"
   },
   "outputs": [],
   "source": [
    "# Install / update spacy\n",
    "# update spacy\n",
    "! pip install -U spacy\n",
    "! python -m spacy info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52896,
     "status": "ok",
     "timestamp": 1615989011511,
     "user": {
      "displayName": "Manuela Paganini",
      "photoUrl": "",
      "userId": "16245217794239939786"
     },
     "user_tz": -60
    },
    "id": "Ua20d8Hkb-jV",
    "outputId": "3cd62ba1-1a5c-4716-d6ba-cd649bb371a8"
   },
   "outputs": [],
   "source": [
    "# load German model\n",
    "! python -m spacy download de_core_news_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5562,
     "status": "ok",
     "timestamp": 1615989351797,
     "user": {
      "displayName": "Manuela Paganini",
      "photoUrl": "",
      "userId": "16245217794239939786"
     },
     "user_tz": -60
    },
    "id": "EpRM7dAgb8Lh"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, unicode_literals\n",
    "import spacy\n",
    "import warnings\n",
    "from spacy.util import minibatch, compounding\n",
    "import de_core_news_lg\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import sample\n",
    "import io, csv\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "from spacy.training import Example\n",
    "from spacy.tokens import Doc\n",
    "from tqdm import tqdm\n",
    "nlp = spacy.load('de_core_news_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 450,
     "status": "ok",
     "timestamp": 1615989353293,
     "user": {
      "displayName": "Manuela Paganini",
      "photoUrl": "",
      "userId": "16245217794239939786"
     },
     "user_tz": -60
    },
    "id": "aPoTwYW1b8Ly"
   },
   "outputs": [],
   "source": [
    "# helper function for incrementing the revision counters\n",
    "def increment_revision_counters(entity_counter, entities):\n",
    "    for entity in entities:\n",
    "        label = entity[2]\n",
    "        if label in entity_counter:\n",
    "            entity_counter[label] += 1\n",
    "        else:\n",
    "            entity_counter[label] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 482,
     "status": "ok",
     "timestamp": 1615989354649,
     "user": {
      "displayName": "Manuela Paganini",
      "photoUrl": "",
      "userId": "16245217794239939786"
     },
     "user_tz": -60
    },
    "id": "7DHliIPnb8L0",
    "outputId": "7f949fdf-1732-4d94-bc9f-6e0f159c50bb"
   },
   "outputs": [],
   "source": [
    "#out_sentences=open(\"input/tagged_sentences.csv\", \"r\").readlines()\n",
    "with open('input/tagged_sentences.json') as json_file:\n",
    "    out_sentences = json.load(json_file)\n",
    "print(\"LENGTH OF DATASET: \",len(out_sentences))\n",
    "dataset_dict={}\n",
    "\n",
    "for sent in out_sentences:\n",
    "    entities = sent[1][\"entities\"]\n",
    "    increment_revision_counters(dataset_dict, entities)\n",
    "#    print(entity[1])\n",
    "    #helper_dict.append(entity[1]['entities'][0][2])\n",
    "#out_sentences[:5]\n",
    "print(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 6680,
     "status": "ok",
     "timestamp": 1615989364176,
     "user": {
      "displayName": "Manuela Paganini",
      "photoUrl": "",
      "userId": "16245217794239939786"
     },
     "user_tz": -60
    },
    "id": "uN4VTGBub8L5",
    "outputId": "6af2c756-6ca4-425c-ae46-22a6bcef5924"
   },
   "outputs": [],
   "source": [
    "npr_df = pd.read_csv(\"external/deu_news_2015_3M-sentences.txt\", delimiter = \"\\t\")\n",
    "npr_df=npr_df.sample(frac=1)\n",
    "npr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 501,
     "status": "ok",
     "timestamp": 1615989369525,
     "user": {
      "displayName": "Manuela Paganini",
      "photoUrl": "",
      "userId": "16245217794239939786"
     },
     "user_tz": -60
    },
    "id": "yiHRyioDb8L-"
   },
   "outputs": [],
   "source": [
    "# *** <- lÃ¶schen?\n",
    "# create an nlp object as we'll use this to seperate the sentences and identify existing entities\n",
    "#loaded already above\n",
    "#nlp = spacy.load('de_core_news_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 173187,
     "status": "ok",
     "timestamp": 1615989581932,
     "user": {
      "displayName": "Manuela Paganini",
      "photoUrl": "",
      "userId": "16245217794239939786"
     },
     "user_tz": -60
    },
    "id": "JSGrNK5hb8MA",
    "outputId": "1098e4ef-a27d-4bd1-8553-297938e2a461"
   },
   "outputs": [],
   "source": [
    "revision_texts = []\n",
    "\n",
    "# convert the articles to spacy objects to better identify the sentences. Disabled unneeded components. # takes ~ 4 minutes\n",
    "for doc in tqdm(nlp.pipe(npr_df.iloc[:100000,1], batch_size=30, disable=[\"tagger\", \"ner\"])):\n",
    "    for sentence in doc.sents:\n",
    "\n",
    "        if  40 < len(sentence.text) < 80:\n",
    "            # some of the sentences had excessive whitespace in between words, so we're trimming that\n",
    "            revision_texts.append(\" \".join(re.split(\"\\s+\", sentence.text, flags=re.UNICODE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EAbowBWMb8MD"
   },
   "outputs": [],
   "source": [
    "revisions = []\n",
    "\n",
    "# Use the existing spaCy model to predict the entities, then append them to revision\n",
    "for doc in nlp.pipe(revision_texts, batch_size=50, disable=[\"tagger\", \"parser\"]):\n",
    "    \n",
    "    # don't append sentences that have no entities\n",
    "    if len(doc.ents) > 0:\n",
    "        revisions.append((doc.text, {\"entities\": [(e.start_char, e.end_char, e.label_) for e in doc.ents]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "74VtWx6Kb8MF",
    "outputId": "04bccf2c-5c76-4dbb-b8cd-5ee2dda1836d"
   },
   "outputs": [],
   "source": [
    "# print an example of the revision sentence\n",
    "print(revisions[0][0])\n",
    "\n",
    "# print an example of the revision data\n",
    "print(revisions[0][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1nkMNnGb8MI"
   },
   "outputs": [],
   "source": [
    "# create arrays to store the revision data\n",
    "TRAIN_REVISION_DATA = []\n",
    "TEST_REVISION_DATA = []\n",
    "\n",
    "# create dictionaries to keep count of the different entities\n",
    "TRAIN_ENTITY_COUNTER = {}\n",
    "TEST_ENTITY_COUNTER = {}\n",
    "\n",
    "# This will help distribute the entities (i.e. we don't want 1000 PERSON entities, but only 80 ORG entities)\n",
    "REVISION_SENTENCE_SOFT_LIMIT = 100\n",
    "\n",
    "\n",
    "\n",
    "random.shuffle(revisions)\n",
    "for revision in revisions:\n",
    "    # get the entities from the revision sentence\n",
    "    entities = revision[1][\"entities\"]\n",
    "\n",
    "    # simple hack to make sure spaCy entities don't get too one-sided\n",
    "    should_append_to_train_counter = 0\n",
    "    for _, _, label in entities:\n",
    "        if label in TRAIN_ENTITY_COUNTER and TRAIN_ENTITY_COUNTER[label] > REVISION_SENTENCE_SOFT_LIMIT:\n",
    "            should_append_to_train_counter -= 1\n",
    "        else:\n",
    "            should_append_to_train_counter += 1\n",
    "\n",
    "    # simple switch for deciding whether to append to train data or test data\n",
    "    if should_append_to_train_counter >= 0:\n",
    "        TRAIN_REVISION_DATA.append(revision)\n",
    "        increment_revision_counters(TRAIN_ENTITY_COUNTER, entities)\n",
    "    else:\n",
    "        TEST_REVISION_DATA.append(revision)\n",
    "        increment_revision_counters(TEST_ENTITY_COUNTER, entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tduOpWdFb8MK",
    "outputId": "be384f69-7334-437e-b5ea-1e90ca8b2546"
   },
   "outputs": [],
   "source": [
    "TRAIN_ENTITY_COUNTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CvUqZc5qb8MM",
    "outputId": "4379c9cd-df81-4cb1-9cd3-f3d1860cf670"
   },
   "outputs": [],
   "source": [
    "TEST_ENTITY_COUNTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Zu_Dpqeb8MR",
    "outputId": "fbeb9a19-be88-433e-de0f-541b99f0b6fa"
   },
   "outputs": [],
   "source": [
    "TRAIN_REVISION_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xUq222v2b8MT"
   },
   "outputs": [],
   "source": [
    "random.shuffle(out_sentences)\n",
    "TRAIN_STAT_DATA=out_sentences[:len(out_sentences)//2]\n",
    "TEST_STAT_DATA=out_sentences[len(out_sentences)//2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yTD4yXbHb8MU",
    "outputId": "e58dfee8-77a7-4eac-c720-ccbb6ecdea5d"
   },
   "outputs": [],
   "source": [
    "print(len(out_sentences))\n",
    "print(len(TRAIN_STAT_DATA))\n",
    "print(len(TEST_STAT_DATA))\n",
    "print(\"REVISION\", len(TRAIN_REVISION_DATA))\n",
    "TRAIN_DATA = TRAIN_REVISION_DATA + TRAIN_STAT_DATA\n",
    "print(\"COMBINED\", len(TRAIN_DATA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9EA9VhQFb8MV",
    "outputId": "3c9bb3ff-b68b-43fe-8df8-440b13076b09"
   },
   "outputs": [],
   "source": [
    "ner = nlp.get_pipe(\"ner\")\n",
    "ner.add_label(\"GRAN\")\n",
    "ner.add_label(\"DATA\")\n",
    "\n",
    "\n",
    "\n",
    "# get the names of the components we want to disable during training\n",
    "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "\n",
    "# start the training loop, only training NER\n",
    "epochs = 30\n",
    "#optimizer = nlp.resume_training()\n",
    "#optimizer = nlp.initialize()\n",
    "with nlp.disable_pipes(*other_pipes), warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"once\", category=UserWarning, module='spacy')\n",
    "    sizes = compounding(1.0, 4.0, 1.001)\n",
    "    \n",
    "    # batch up the examples using spaCy's minibatc\n",
    "    for epoch in range(epochs):\n",
    "        random.shuffle(TRAIN_DATA)\n",
    "        #text = []\n",
    "        #annots=[]\n",
    "        examples=[]\n",
    "\n",
    "\n",
    "        for text,annots in TRAIN_DATA:\n",
    "            #text.append(t)\n",
    "            #annots.append(a)\n",
    "            doc = nlp.make_doc(text)    \n",
    "            example = Example.from_dict(doc, annots)\n",
    "            examples.append(example)\n",
    "        \n",
    "        losses = {}\n",
    "        \n",
    "        nlp.update(examples, drop=0.35, losses=losses)#,sgd=optimizer)\n",
    "\n",
    "        print(\"Losses ({}/{})\".format(epoch + 1, epochs), losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7_RIUKQTb8MX",
    "outputId": "0e72e1ec-5e98-4ec9-f067-cff553543ccd",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "statbot_colors = {\"GRAN\": \"linear-gradient(90deg, #aa9cfc, #fc9ce7)\",\n",
    "                  \"DATA\": \"linear-gradient(90deg, #ffff00, #ff8c00)\"}\n",
    "statbot_options = {\"ents\": [\"PER\",\"LOC\",\"ORG\",\"MISC\",\"GRAN\",\"DATA\"], \"colors\": statbot_colors}\n",
    "spacy.displacy.render(nlp(\"Ich heisse Christian und war heute in ZÃ¼rich bei IBM im Internet.\"), style=\"ent\",options=statbot_options)\n",
    "spacy.displacy.render(nlp(\"Wie viele KÃ¼he hat die Gemeinde BÃ¼lach?\"), style=\"ent\",options=statbot_options)\n",
    "spacy.displacy.render(nlp(\"Wie hoch ist Eigenkapital auf Bezirksebene?\"), style=\"ent\",options=statbot_options)\n",
    "spacy.displacy.render(nlp(\"Ich brauche die Daten pro Bezirk\"), style=\"ent\",options=statbot_options)\n",
    "spacy.displacy.render(nlp(\"Ich brauche die Daten fÃ¼r den gesamten Kanton.\"), style=\"ent\",options=statbot_options)\n",
    "spacy.displacy.render(nlp(\"Wie viel Bauinv. EFH 5 Jahre  hat  in Regensdorf  ?\"), style=\"ent\",options=statbot_options)\n",
    "spacy.displacy.render(nlp(\"Was ist der Anteil an MIV-Anteil (Modal Split)   auf Bezirksebene ?\"), style=\"ent\",options=statbot_options)\n",
    "spacy.displacy.render(nlp(\"Was ist der Anteil an Geb.Vol. Dienstleistungen: Zunahme   in Flaach ?\"), style=\"ent\",options=statbot_options)\n",
    "spacy.displacy.render(nlp(\"Welches ist das SchÃ¼l. Sekundarstufe II   fÃ¼r den gesamten Kanton ?\"), style=\"ent\",options=statbot_options)\n",
    "spacy.displacy.render(nlp(\"Welche Gemeinde hat die grÃ¶sste BevÃ¶lkerung?\"), style=\"ent\",options=statbot_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "owtuVlcab8MY",
    "outputId": "b5496a10-135e-4e89-abe5-2024538153d9"
   },
   "outputs": [],
   "source": [
    "# dictionary to hold our evaluation data\n",
    "stat_evaluation = {\n",
    "    \"GRAN\": {\n",
    "        \"correct\": 0,\n",
    "        \"total\": 0,\n",
    "    },\n",
    "    \"DATA\": {\n",
    "        \"correct\": 0,\n",
    "        \"total\": 0,\n",
    "    }\n",
    "}\n",
    "\n",
    "word_evaluation = {\n",
    "    \"GRAN\": {\n",
    "        \"correct\": 0,\n",
    "        \"total\": 0\n",
    "    },\n",
    "    \"DATA\": {\n",
    "        \"correct\": 0,\n",
    "        \"total\": 0,\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "for stat in TEST_STAT_DATA:\n",
    "    # extract the sentence and correct stat entities according to our test data\n",
    "    sentence = stat[0]\n",
    "    entities = stat[1][\"entities\"]\n",
    "\n",
    "    # for each entity, use our updated model to make a prediction on the sentence\n",
    "    for entity in entities:\n",
    "        doc = nlp(sentence)\n",
    "        correct_text = sentence[entity[0]:entity[1]]\n",
    "        n_worded_stat =  len(correct_text.split())\n",
    "        print(n_worded_stat)\n",
    "\n",
    "        # if we find that there's a match for predicted entity and predicted text, increment correct counters\n",
    "        for ent in doc.ents:\n",
    "            print(\"ENT_LABEL\",ent.label_)\n",
    "            print(\"ENTITY2\",entity[2])\n",
    "            print(\"ENT_TEXT\",ent.text)\n",
    "            print(\"CORRECT:TEXT\",correct_text)\n",
    "            if ent.label_ == entity[2] and ent.text == correct_text:\n",
    "                \n",
    "                stat_evaluation[entity[2]][\"correct\"] += 1\n",
    "                if n_worded_stat > 0:\n",
    "                    word_evaluation[entity[2]][\"correct\"] += 1\n",
    "\n",
    "                # this break is important, ensures that we're not double counting on a correct match\n",
    "                break\n",
    "\n",
    "        #  increment total counters after each entity loop\n",
    "        stat_evaluation[entity[2]][\"total\"] += 1\n",
    "        if n_worded_stat > 0:\n",
    "            word_evaluation[entity[2]][\"total\"] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VzZ4HP6Eb8Mb",
    "outputId": "96b348ac-104d-4875-a390-0e00347914ca"
   },
   "outputs": [],
   "source": [
    "for key in word_evaluation:\n",
    "    correct = word_evaluation[key][\"correct\"]\n",
    "    total = word_evaluation[key][\"total\"]\n",
    "\n",
    "    print(f\"{key}: {correct / total * 100:.2f}%\")\n",
    "\n",
    "stat_total_sum = 0\n",
    "stat_correct_sum = 0\n",
    "\n",
    "print(\"---\")\n",
    "for key in stat_evaluation:\n",
    "    correct = stat_evaluation[key][\"correct\"]\n",
    "    total = stat_evaluation[key][\"total\"]\n",
    "    \n",
    "    stat_total_sum += total\n",
    "    stat_correct_sum += correct\n",
    "\n",
    "    print(f\"{key}: {correct / total * 100:.2f}%\")\n",
    "\n",
    "print(f\"\\nTotal: {stat_correct_sum/stat_total_sum * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3fhfB65gb8Md"
   },
   "outputs": [],
   "source": [
    "# dictionary which will be populated with the entities and result information\n",
    "entity_evaluation = {}\n",
    "\n",
    "# helper function to udpate the entity_evaluation dictionary\n",
    "def update_results(entity, metric):\n",
    "    if entity not in entity_evaluation:\n",
    "        entity_evaluation[entity] = {\"correct\": 0, \"total\": 0}\n",
    "    \n",
    "    entity_evaluation[entity][metric] += 1\n",
    "\n",
    "# same as before, see if entities from test set match what spaCy currently predicts\n",
    "for data in TEST_REVISION_DATA:\n",
    "    sentence = data[0]\n",
    "    entities = data[1][\"entities\"]\n",
    "\n",
    "    for entity in entities:\n",
    "        doc = nlp(sentence)\n",
    "        correct_text = sentence[entity[0]:entity[1]]\n",
    "\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == entity[2] and ent.text == correct_text:\n",
    "                update_results(ent.label_, \"correct\")\n",
    "                break\n",
    "\n",
    "        update_results(entity[2], \"total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r34g33cob8Mf"
   },
   "outputs": [],
   "source": [
    "sum_total = 0\n",
    "sum_correct = 0\n",
    "\n",
    "for entity in entity_evaluation:\n",
    "    total = entity_evaluation[entity][\"total\"]\n",
    "    correct = entity_evaluation[entity][\"correct\"]\n",
    "\n",
    "    sum_total += total\n",
    "    sum_correct += correct\n",
    "    \n",
    "    print(\"{} | {:.2f}%\".format(entity, correct / total * 100))\n",
    "\n",
    "print()\n",
    "print(\"Overall accuracy: {:.2f}%\".format(sum_correct / sum_total * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NgCG-oqEb8Mj"
   },
   "outputs": [],
   "source": [
    "nlp.meta[\"name\"] = \"stat_entity_extractor_v1\"\n",
    "nlp.to_disk(\"./models/v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rX0e1rXqb8Mz"
   },
   "outputs": [],
   "source": [
    "TRAIN_STAT_DATA[5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EY1eJev0b8M0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc = nlp(u'Welche Gemeinde hat die grÃ¶sste BevÃ¶lkerung und welche hatte im 2019 den hÃ¶chsten AuslÃ¤nderanteil?')\n",
    "\n",
    "# show universal pos tags\n",
    "print(' '.join('{word}/{tag}'.format(word=t.orth_, tag=t.pos_) for t in doc))\n",
    "# output: Ich/PRON bin/AUX ein/DET Berliner/NOUN ./PUNCT\n",
    "\n",
    "# show German specific pos tags (STTS)\n",
    "print(' '.join('{word}/{tag}'.format(word=t.orth_, tag=t.tag_) for t in doc))\n",
    "# output: Ich/PPER bin/VAFIN ein/ART Berliner/NN ./$.\n",
    "\n",
    "# show dependency arcs\n",
    "print('\\n'.join('{child:<8} <{label:-^7} {head}'.format(child=t.orth_, label=t.dep_, head=t.head.orth_) for t in doc))\n",
    "# output: (sb: subject, nk: noun kernel, pd: predicate)\n",
    "\n",
    "#named entities\n",
    "print(\"Named Entity Recognition:\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text)\n",
    "print(\"Noun chunks:\")\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aKoFE6FNb8M1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "A03_train.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
