{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xM3TUnWwZutX"
   },
   "source": [
    "# 02 Generate and tag sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connection to Google Drive - Only execute if you are running this on GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36056,
     "status": "ok",
     "timestamp": 1616052881434,
     "user": {
      "displayName": "Manuela Paganini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjkZTZ9-xVoCs5dn8iyeQ0Mnibr5RzCWK7-5BvvIg=s64",
      "userId": "08508102103238796518"
     },
     "user_tz": -60
    },
    "id": "W_16I4-pZrqo",
    "outputId": "e0e57be1-67e2-49b6-d801-ab991ba4dbcd"
   },
   "outputs": [],
   "source": [
    "# set up connection to your google drive\n",
    "# please click on the link generated and enter the authorisation code\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1485,
     "status": "ok",
     "timestamp": 1616052892890,
     "user": {
      "displayName": "Manuela Paganini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjkZTZ9-xVoCs5dn8iyeQ0Mnibr5RzCWK7-5BvvIg=s64",
      "userId": "08508102103238796518"
     },
     "user_tz": -60
    },
    "id": "SFoJXAS5afk8",
    "outputId": "118c2ed0-9df5-4689-b482-339feeaa95d8"
   },
   "outputs": [],
   "source": [
    "cd /content/drive/MyDrive/Colab/statbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3270,
     "status": "ok",
     "timestamp": 1615991295329,
     "user": {
      "displayName": "Manuela Paganini",
      "photoUrl": "",
      "userId": "16245217794239939786"
     },
     "user_tz": -60
    },
    "id": "PokqyErnZrY4",
    "outputId": "2d38995c-6143-4d15-ba06-094ac80bbb85"
   },
   "outputs": [],
   "source": [
    "! pip install wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PUEm1CwIW77T"
   },
   "source": [
    "#### Skript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WNDOagyTW6bz"
   },
   "outputs": [],
   "source": [
    "# DISCLAIMER: Large part of this code for training new entities based on a pre-trained model, \n",
    "# is taken from Isaac Aderogba under the following link https://deepnote.com/publish/2cc2d19c-c3ac-4321-8853-0bcf2ef565b3\n",
    "# \n",
    "# Statistics Canton Zurich humbly tried to adapt his code to our purposes of training\n",
    "#statistically relevant entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ES7xxOhwW6cA"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, unicode_literals\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import sample\n",
    "import io, csv\n",
    "import re\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ui2KdJ0lW6cC"
   },
   "outputs": [],
   "source": [
    "def uniform_cleaning(str_in):\n",
    "    str_in=str_in.replace(\"-\",\"\")\n",
    "    str_in=str_in.replace(\"(\",\"\")\n",
    "    str_in=str_in.replace(\")\",\"\")\n",
    "    str_in=str_in.replace(\"ü\",\"ue\")\n",
    "    str_in=str_in.replace(\"ä\",\"ae\")\n",
    "    str_in=str_in.replace(\"ö\",\"oe\")\n",
    "    return(str_in)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 99040,
     "status": "ok",
     "timestamp": 1615991404580,
     "user": {
      "displayName": "Manuela Paganini",
      "photoUrl": "",
      "userId": "16245217794239939786"
     },
     "user_tz": -60
    },
    "id": "tc4A3oDYW6cD",
    "outputId": "eadb3f0e-99d3-4b07-ca46-c4cfb20b6e86"
   },
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "\n",
    "df=pd.read_csv(\"data/datasets_overview.csv\")\n",
    "\n",
    "#one more prepatation\n",
    "df['question_type'] = df['dataset_title'].str.extract(r\"\\[(.*?)\\]\", expand=False)\n",
    "df['question_type'] = np.where(df['question_type']== '%', \"percent\", \"cardinal\")\n",
    "df['dataset_title'] = df['dataset_title'].str.replace(r\"\\[(.*?)\\]\", \"\")\n",
    "#structurize relevant informations such as variables\n",
    "\n",
    "#load content of datasets\n",
    "\n",
    "\n",
    "\n",
    "sentences=pd.read_csv(\"input/question_generator.csv\")\n",
    "out_sentences=[]\n",
    "\n",
    "for i in df['index'].unique():\n",
    "    filename=\"data/\"+str(i)+\".csv\"\n",
    "    with io.open(filename, 'r', encoding=\"latin-1\") as csvfile:\n",
    "        dialect = csv.Sniffer().sniff(csvfile.readline(), [',',';'])\n",
    "        csvfile.seek(0)\n",
    "        mydelimiter=dialect.delimiter\n",
    "    this_data=pd.read_csv(filename,delimiter=dialect.delimiter)\n",
    "    vars=df['var'].loc[df['index'] == i]\n",
    "    dataset_title=df['dataset_title'].loc[df['index']==i].any()\n",
    "    question_type=df['question_type'].loc[df['index']==i].any()\n",
    "    if vars.str.contains(\"INDIKATOR_VALUE\").sum():\n",
    "        main=uniform_cleaning(dataset_title.strip())\n",
    "        filter_vars=vars\n",
    "        #print(filter_vars)\n",
    "    else:\n",
    "        continue #temporary so that all variables look alike\n",
    "        print(\"ATTRIBUTING MAIN VARIABLE TO: \",dataset_title)\n",
    "        length_title_string=len(dataset_title.split())\n",
    "        highest_similarity=0\n",
    "        which=None\n",
    "        for var in vars:\n",
    "            temp_string=dataset_title+\" \"+var\n",
    "            #print(temp_string)\n",
    "            doc = nlp(temp_string)\n",
    "            assert len(doc.vector) == len(doc[0].vector)\n",
    "            calc_similarity=doc[:length_title_string].similarity(doc[length_title_string:])\n",
    "            if  calc_similarity> highest_similarity:\n",
    "                highest_similarity=calc_similarity\n",
    "                which=var\n",
    "\n",
    "        print(\"Highest similarity:\",var,highest_similarity)\n",
    "        main=uniform_cleaning(var)\n",
    "        vars=vars.tolist()\n",
    "        print(vars)\n",
    "        print(main)\n",
    "        filter_vars=vars.remove(main)\n",
    "    #the following temporary because it is standardized\n",
    "    filter_vars=list(filter_vars)\n",
    "    filter_vars.append(\"\")\n",
    "    try:\n",
    "        filter_vars.remove(\"INDIKATOR_JAHR\")\n",
    "        filter_vars.remove(\"GEBIET_NAME\")\n",
    "        filter_vars.remove(\"BFS_NR\")\n",
    "        filter_vars.remove(\"INDIKATOR_VALUE\")\n",
    "    except:\n",
    "        print(\"variable removing empty\")\n",
    "\n",
    "    for sentence in sentences['question'].loc[sentences['main_type'] == question_type]:\n",
    "        out_entities = []\n",
    "        \n",
    "        sentence=sentence.replace(\"{main}\",main)\n",
    "        sentence=sentence.replace(\"{localitylevel}\",\"\")#at the moment empty\n",
    "        #TODO either one locality, one level, or several localities\n",
    "        random_value=sample([\"one locality\",\"one level\",\"several localities\"],1)[0]\n",
    "        if random_value==\"one locality\":\n",
    "            locality_insert=\"in \"+sample(list(this_data['GEBIET_NAME']),1)[0]\n",
    "        if random_value==\"one level\":\n",
    "            locality_insert=sample([\"für den gesamten Kanton\",\"im Kanton Zürich\",\"auf Bezirksebene\",\n",
    "            \"für alle Bezirke\",\"pro Bezirk\",\"auf Gemeindeebene\",\"für alle Gemeinden\",\"pro Gemeinde\"],1)[0]\n",
    "        if random_value==\"several localities\":\n",
    "            locality_insert=\"\"\n",
    "            local_loop=sample([1,2,3],1)[0]\n",
    "            for local in range(0,local_loop):\n",
    "                if local!=0 and local!=(local_loop-1):\n",
    "                    locality_insert+=\", \"\n",
    "                if local!=0 and local==(local_loop-1):\n",
    "                    locality_insert+=\" und \"\n",
    "                locality_insert+=sample(list(this_data['GEBIET_NAME']),1)[0]\n",
    "        sentence=sentence.replace(\"{locality}\",locality_insert)\n",
    "        sentence=sentence.replace(\"{yeartime}\",\"\")#TODO no,aktuellste,neuste, value from list, from to year\n",
    "        sentence=sentence.replace(\"{filter}\",sample(list(filter_vars),1)[0])\n",
    "\n",
    "        for mat in re.findall(r'.*?\\[(.*)].*', sentence):\n",
    "            which_part=sample([1,2],1)\n",
    "\n",
    "            if which_part==1:\n",
    "                sentence=sentence.replace(\"[\"+mat+\"]\",mat.partition(\"|\")[0])\n",
    "                #sentence=re.sub(\"[\"+mat+\"]\",mat.partition(\"|\")[0],sentence)\n",
    "            else:\n",
    "                sentence=sentence.replace(\"[\"+mat+\"]\",mat.partition(\"|\")[2])\n",
    "                #sentence=re.sub(\"[\"+mat+\"]\",mat.partition(\"|\")[2],sentence)\n",
    "                \n",
    "        \n",
    "        #now the symbol - has to be deleted as it gives issues\n",
    "        sentence=uniform_cleaning(sentence)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        #### TAGGING\n",
    "\n",
    "        #1) GRAN\n",
    "        match_span = re.search(r'\\bGEMEINDE\\b|\\bGEMEINDEN\\b|\\bGEMEINDEEBENE\\b|\\bBEZIRK\\b|\\bBEZIRKSEBENE\\b|\\bBEZIRKE\\b|\\bKANTON\\b|\\bKANTONSEBENE\\b|\\bREGION\\b', sentence,flags=re.IGNORECASE)\n",
    "        if match_span is not None:\n",
    "            match_span=match_span.span()\n",
    "            out_entities.append((match_span[0], match_span[1], \"GRAN\"))\n",
    "            \n",
    "        #2) DATASET\n",
    "        print(main)\n",
    "        match_span = re.search(main,sentence,flags=re.IGNORECASE) \n",
    "        print(match_span)\n",
    "        if match_span is not None:\n",
    "            match_span=match_span.span()\n",
    "            out_entities.append((match_span[0], match_span[1], \"DATA\"))\n",
    "            \n",
    "            \n",
    "        \n",
    "\n",
    "        out_sentences.append((sentence, {\"entities\": out_entities}))\n",
    "        \n",
    "        \n",
    "        \n",
    "       \n",
    "                \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 525,
     "status": "ok",
     "timestamp": 1615991411567,
     "user": {
      "displayName": "Manuela Paganini",
      "photoUrl": "",
      "userId": "16245217794239939786"
     },
     "user_tz": -60
    },
    "id": "nQGgYfIlW6cH",
    "outputId": "500dc122-63b3-4cce-cb75-79fa9cfcc5f2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(out_sentences[:10])\n",
    "with open(\"input/tagged_sentences.json\",\"w\",encoding='utf-8') as outfile:\n",
    "    json.dump(out_sentences, outfile, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 477,
     "status": "ok",
     "timestamp": 1615991420511,
     "user": {
      "displayName": "Manuela Paganini",
      "photoUrl": "",
      "userId": "16245217794239939786"
     },
     "user_tz": -60
    },
    "id": "LThy3_cuW6cK",
    "outputId": "845e2c27-e4dd-4c02-a307-992afa7b8d05"
   },
   "outputs": [],
   "source": [
    "print(out_sentences[1])\n",
    "print(out_sentences[1][0][10])\n",
    "print(out_sentences[1][0][12-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "saOxPe9BW6cM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "A02_generate_and_tag_sentences.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
