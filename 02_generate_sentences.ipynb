{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISCLAIMER: Large part of this code for training new entities based on a pre-trained model, \n",
    "# is taken from Isaac Aderogba under the following link https://deepnote.com/publish/2cc2d19c-c3ac-4321-8853-0bcf2ef565b3\n",
    "# \n",
    "# Statistics Canton Zurich humbly tried to adapt his code to our purposes of training\n",
    "#statistically relevant entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, unicode_literals\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import sample\n",
    "import io, csv\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-16b32060e73c>:6: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['dataset_title'] = df['dataset_title'].str.replace(r\"\\[(.*?)\\]\", \"\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable removing empty\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df=pd.read_csv(\"data/datasets_overview.csv\")\n",
    "\n",
    "#one more prepatation\n",
    "df['question_type'] = df['dataset_title'].str.extract(r\"\\[(.*?)\\]\", expand=False)\n",
    "df['question_type'] = np.where(df['question_type']== '%', \"percent\", \"cardinal\")\n",
    "df['dataset_title'] = df['dataset_title'].str.replace(r\"\\[(.*?)\\]\", \"\")\n",
    "#structurize relevant informations such as variables\n",
    "\n",
    "#load content of datasets\n",
    "\n",
    "\n",
    "\n",
    "sentences=pd.read_csv(\"input/question_generator.csv\")\n",
    "out_sentences=[]\n",
    "\n",
    "for i in df['index'].unique():\n",
    "    filename=\"data/\"+str(i)+\".csv\"\n",
    "    with io.open(filename, 'r', encoding=\"latin-1\") as csvfile:\n",
    "        dialect = csv.Sniffer().sniff(csvfile.readline(), [',',';'])\n",
    "        csvfile.seek(0)\n",
    "        mydelimiter=dialect.delimiter\n",
    "    this_data=pd.read_csv(filename,delimiter=dialect.delimiter)\n",
    "    vars=df['var'].loc[df['index'] == i]\n",
    "    dataset_title=df['dataset_title'].loc[df['index']==i].any()\n",
    "    question_type=df['question_type'].loc[df['index']==i].any()\n",
    "    if vars.str.contains(\"INDIKATOR_VALUE\").sum():\n",
    "        main=dataset_title\n",
    "        filter_vars=vars\n",
    "        #print(filter_vars)\n",
    "    else:\n",
    "        continue #temporary so that all variables look alike\n",
    "        print(\"ATTRIBUTING MAIN VARIABLE TO: \",dataset_title)\n",
    "        length_title_string=len(dataset_title.split())\n",
    "        highest_similarity=0\n",
    "        which=None\n",
    "        for var in vars:\n",
    "            temp_string=dataset_title+\" \"+var\n",
    "            #print(temp_string)\n",
    "            doc = nlp(temp_string)\n",
    "            assert len(doc.vector) == len(doc[0].vector)\n",
    "            calc_similarity=doc[:length_title_string].similarity(doc[length_title_string:])\n",
    "            if  calc_similarity> highest_similarity:\n",
    "                highest_similarity=calc_similarity\n",
    "                which=var\n",
    "\n",
    "        print(\"Highest similarity:\",var,highest_similarity)\n",
    "        main=var\n",
    "        vars=vars.tolist()\n",
    "        print(vars)\n",
    "        print(main)\n",
    "        filter_vars=vars.remove(main)\n",
    "    #the following temporary because it is standardized\n",
    "    filter_vars=list(filter_vars)\n",
    "    filter_vars.append(\"\")\n",
    "    try:\n",
    "        filter_vars.remove(\"INDIKATOR_JAHR\")\n",
    "        filter_vars.remove(\"GEBIET_NAME\")\n",
    "        filter_vars.remove(\"BFS_NR\")\n",
    "        filter_vars.remove(\"INDIKATOR_VALUE\")\n",
    "    except:\n",
    "        print(\"variable removing empty\")\n",
    "\n",
    "    for sentence in sentences['question'].loc[sentences['main_type'] == question_type]:\n",
    "        \n",
    "        sentence=sentence.replace(\"{main}\",main)\n",
    "        sentence=sentence.replace(\"{localitylevel}\",\"\")#at the moment empty\n",
    "        #TODO either one locality, one level, or several localities\n",
    "        random_value=sample([\"one locality\",\"one level\",\"several localities\"],1)[0]\n",
    "        if random_value==\"one locality\":\n",
    "            locality_insert=\"in \"+sample(list(this_data['GEBIET_NAME']),1)[0]\n",
    "        if random_value==\"one level\":\n",
    "            locality_insert=sample([\"für den gesamten Kanton\",\"im Kanton Zürich\",\"auf Bezirksebene\",\n",
    "            \"für alle Bezirke\",\"pro Bezirk\",\"auf Gemeindeebene\",\"für alle Gemeinden\",\"pro Gemeinde\"],1)[0]\n",
    "        if random_value==\"several localities\":\n",
    "            locality_insert=\"\"\n",
    "            local_loop=sample([1,2,3],1)[0]\n",
    "            for local in range(0,local_loop):\n",
    "                if local!=0 and local!=(local_loop-1):\n",
    "                    locality_insert+=\", \"\n",
    "                if local!=0 and local==(local_loop-1):\n",
    "                    locality_insert+=\" und \"\n",
    "                locality_insert+=sample(list(this_data['GEBIET_NAME']),1)[0]\n",
    "        sentence=sentence.replace(\"{locality}\",locality_insert)\n",
    "        sentence=sentence.replace(\"{yeartime}\",\"\")#TODO no,aktuellste,neuste, value from list, from to year\n",
    "        sentence=sentence.replace(\"{filter}\",sample(list(filter_vars),1)[0])\n",
    "\n",
    "        for mat in re.findall(r'.*?\\[(.*)].*', sentence):\n",
    "            which_part=sample([1,2],1)\n",
    "\n",
    "            if which_part==1:\n",
    "                sentence=sentence.replace(\"[\"+mat+\"]\",mat.partition(\"|\")[0])\n",
    "                #sentence=re.sub(\"[\"+mat+\"]\",mat.partition(\"|\")[0],sentence)\n",
    "            else:\n",
    "                sentence=sentence.replace(\"[\"+mat+\"]\",mat.partition(\"|\")[2])\n",
    "                #sentence=re.sub(\"[\"+mat+\"]\",mat.partition(\"|\")[2],sentence)\n",
    "                \n",
    "        out_sentences.append(sentence)\n",
    "                \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wie viel Eigenkapital  hat  in Bauma  ?', 'Welches ist das Eigenkapital   in Marthalen ?', 'Was ist der Anteil an Bruttoverschuldungsanteil   in Kleinandelfingen ?', 'Wie viel Steuerbares Vermögen natürliche Pers.  hat  Birmensdorf  ?', 'Welches ist das Steuerbares Vermögen natürliche Pers.   in Rorbas ?']\n"
     ]
    }
   ],
   "source": [
    "print(out_sentences[:5])\n",
    "pd.DataFrame(out_sentences).to_csv(\"input/generated_sentences.csv\",index=False,header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".statbot",
   "language": "python",
   "name": ".statbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
